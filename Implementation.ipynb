{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import socket\n",
    "import ssl\n",
    "import datetime\n",
    "from urllib.parse import urlparse\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re \n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ssl_check():\n",
    "             \n",
    "    #returns the duration of the ssl certificate\n",
    "    def getDuration(self,hostname):\n",
    "    \n",
    "        self.duration = 0\n",
    "        ssl_date_fmt = r'%b %d %H:%M:%S %Y %Z'\n",
    "        context = ssl.create_default_context()\n",
    "        conn = context.wrap_socket(socket.socket(socket.AF_INET),server_hostname=hostname,)\n",
    "        conn.settimeout(3.0)\n",
    "        conn.connect((hostname, 443))\n",
    "        ssl_info = conn.getpeercert()\n",
    "        Start_ON = datetime.datetime.strptime(ssl_info['notBefore'], ssl_date_fmt) \n",
    "        Exp_ON = datetime.datetime.strptime(ssl_info['notAfter'], ssl_date_fmt)\n",
    "        Days_Remaining = Exp_ON - datetime.datetime.utcnow()\n",
    "        self.duration = Exp_ON-Start_ON\n",
    "        return self.duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class URL():\n",
    "    \n",
    "    \"\"\"\n",
    "    class to extract all the eight features we need\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,url):\n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        Features we need to find value of:\n",
    "        'SSLfinal_State',  'URL_of_Anchor',  'Prefix_Suffix',  'web_traffic',  \n",
    "        'having_Sub_Domain', 'Request_URL', 'Links_in_tags', 'SFH'.\n",
    "        \n",
    "        \"\"\"\n",
    "        #attributes\n",
    "        self.flag = False\n",
    "        self.SSLfinal_State = -2\n",
    "        self.URL_of_Anchor = -2\n",
    "        self.Prefix_Suffix = -2\n",
    "        self.web_traffic = -2\n",
    "        self.having_Sub_Domain = -2\n",
    "        self.Request_URL = -2\n",
    "        self.Links_in_tags = -2\n",
    "        self.SFH = -2\n",
    "        \n",
    "        #methods\n",
    "        url = self.findHREF(url)\n",
    "        if url:\n",
    "            self.flag = True\n",
    "            self.findSSLPreSufSubDomain(url)          #gets the value of features: 'SSLfinal_state' , 'Prefix_Suffix', 'having_Sub_Domain'.\n",
    "            self.findWebTraffic(url)                  #gets the value of feature: 'web_traffic'.\n",
    "            self.findRequestURLandURLofAnchor(url)    #gets the value of features: 'Request_URL' , 'URL_of_Anchor'\n",
    "            self.findLinksInTags(url)                 #gets the value of feature: 'Links_in_tags'\n",
    "            self.findSFH(url)                          #gets the value of feature: 'SFH'\n",
    "\n",
    "        else:\n",
    "            print(\"!! Please feed a proper URL !!\")\n",
    "            \n",
    "    def getFlag(self):\n",
    "        return self.flag\n",
    "    def findDomain(self,url):\n",
    "        \n",
    "        \"\"\"\n",
    "        returns the domain name in url\n",
    "        Example: https://www.netflix.com/watch/80028080?trackId=155573560\n",
    "        output: netflix.com\n",
    "        \n",
    "        \"\"\"\n",
    "        parsed_uri = urlparse(url)\n",
    "        #a URL consists: scheme://netloc/path;parameters?query#fragment\n",
    "        #the netloc part contains what we need: domain\n",
    "        result =  '{uri.netloc}'.format(uri=parsed_uri)\n",
    "        if 'www' in result:\n",
    "            result = result.split('www.')[1] \n",
    "        return result\n",
    "        \n",
    "        \n",
    "    def findHREF(self,string):  \n",
    "        \n",
    "        \"\"\"\n",
    "        finds link in a string using regular expression and returns it.\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        regex = r\"(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\\\".,<>?«»“”‘’]))\"\n",
    "        url = re.findall(regex,string)\n",
    "        #print(url)\n",
    "        if url:\n",
    "            url =  [x[0] for x in url]\n",
    "            url = str(url[0])\n",
    "            return url\n",
    "        \n",
    "        \n",
    "    def findSSLPreSufSubDomain(self,url): \n",
    "        \n",
    "        \"\"\"\n",
    "        For feature 'SSLfinal_State' :\n",
    "        Use https and Issuer Is Trusted and Age of Certificate ≥ 1 Years → Legitimate : 1\n",
    "        Using https and Issuer Is Not Trusted → Suspicious : 0\n",
    "        Otherwise → Phishing : -1\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "        For feature 'having_Sub_Domain' :\n",
    "        Dots In Domain Part = 2 → Legitimate : 1\n",
    "        Dots In Domain Part = 3 → Suspicious : 0\n",
    "        Otherwise → Phishing : -1\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "        For feature 'Prefix_Suffix' :\n",
    "        Domain Name Part Includes (−) Symbol → Phishing : -1\n",
    "        Otherwise → Legitimate : 1\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        result = self.findDomain(url)\n",
    "        #print(result)\n",
    "        if result:\n",
    "            #print(\"flag1\")\n",
    "            #print(result)\n",
    "            if 'https' in result or 'http' in result:\n",
    "                #if http/http is present in domain name (which should not be present as protocols are not included in domain)\n",
    "                SSLfinal_State = -1\n",
    "                sub_domain = -1\n",
    "                prefix_suffix = -1\n",
    "                #print(\"flag2\")\n",
    "            \n",
    "            else:\n",
    "                try:\n",
    "                    ssl = ssl_check()\n",
    "                    #getting the duration of the ssl certificate\n",
    "                    duration = ssl.getDuration(result)\n",
    "                    #print(duration.days)\n",
    "                    if duration.days >= 365:\n",
    "                        SSLfinal_State = 1\n",
    "                    else:\n",
    "                        SSLfinal_State = 0\n",
    "                    if '-' in result:\n",
    "                        prefix_suffix = -1\n",
    "                    else:\n",
    "                        prefix_suffix = 1\n",
    "                    if result.count('.') > 3:\n",
    "                        sub_domain = -1\n",
    "                    elif result.count('.') == 3:\n",
    "                        sub_domain = 0\n",
    "                    else:\n",
    "                        sub_domain = 1\n",
    "                    #print(\"flag3\")\n",
    "                except:\n",
    "                    SSLfinal_State = -1\n",
    "                    prefix_suffix = -1\n",
    "                    sub_domain = 0\n",
    "            \n",
    "                    \n",
    "        else:\n",
    "            SSLfinal_State = -1\n",
    "            prefix_suffix = -1\n",
    "            sub_domain = 0\n",
    "            \n",
    "        #print('SSLfinal_State',SSLfinal_State)\n",
    "        #print('Prefix-Suffix',prefix_suffix)\n",
    "        #print('Sub_Domain',sub_domain)\n",
    "        \n",
    "        self.SSLfinal_State = SSLfinal_State\n",
    "        self.Prefix_Suffix = prefix_suffix\n",
    "        self.having_Sub_Domain = sub_domain\n",
    "        \n",
    "    def findWebTraffic(self, url):\n",
    "        \n",
    "        \"\"\"\n",
    "        Website Rank < 100,000 → Legitimate : 1 \n",
    "        Website Rank > 100,000 → Suspicious : 0\n",
    "        Otherwise → Phishing : -1\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        #print(url)\n",
    "        domain = self.findDomain(url)\n",
    "        #print(domain)\n",
    "        try:\n",
    "            request = requests.get(\"https://www.alexa.com/siteinfo/\" +domain+'#section_traffic')\n",
    "            soup = BeautifulSoup(request.content,'html.parser')\n",
    "            content = str(soup.head.script)\n",
    "            #global contains the rank of the website. Hence, extracting the value of 'global'\n",
    "            content = content.split(\"global\")[1]\n",
    "            content = content.split(' ')[1]\n",
    "            content = content.split(',')\n",
    "            rank = content[0]\n",
    "            #print(rank)\n",
    "        \n",
    "            if rank != 'false':\n",
    "                rank = int(rank)\n",
    "                if rank <= 100000:\n",
    "                    web_traffic = 1\n",
    "                else:\n",
    "                    web_traffic = 0\n",
    "            else:\n",
    "                web_traffic = -1\n",
    "        except:\n",
    "            web_traffic = -1\n",
    "        \n",
    "        self.web_traffic = web_traffic\n",
    "        \n",
    "    \n",
    "    def findRequestURLandURLofAnchor(self, url):\n",
    "        \n",
    "        \"\"\"\n",
    "        For feature 'Request_URL' :\n",
    "        % of Request URL < 22% → Legitimate : 1\n",
    "        % of Request URL ≥ 22% and 61% → Suspicious : 0\n",
    "        Otherwise → feature = Phishing : -1\n",
    "        \n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        For feature 'URL_of_Anchor' :\n",
    "        % of URL Of Anchor < 31% → 𝐿𝑒𝑔𝑖𝑡𝑖𝑚𝑎𝑡𝑒 : 1\n",
    "        % of URL Of Anchor ≥ 31% And ≤ 67% → Suspicious : 0\n",
    "        Otherwise → Phishing : -1\n",
    "        \n",
    "        \"\"\"\n",
    "        #Checking Request URLs\n",
    "        domain = self.findDomain(url)\n",
    "        #print(domain)\n",
    "        try:\n",
    "            req = requests.get(url)\n",
    "            soup = BeautifulSoup(req.content, 'html.parser',from_encoding=\"iso-8859-1\")\n",
    "            #from_encoding=\"iso-8859-1\"\n",
    "            images = soup.find_all('img')\n",
    "            videos = soup.find_all('video')\n",
    "            audios = soup.find_all('audio')\n",
    "            sources = soup.find_all('source')\n",
    "            \n",
    "            anchors = soup.find_all('a')\n",
    "            anchor_count = len(anchors)\n",
    "            anchor_phish = 0\n",
    "\n",
    "            count = len(images) + len(videos) + len(audios) + len(sources)\n",
    "            phish = 0\n",
    "\n",
    "            for i in images:\n",
    "                try:\n",
    "                    ans = self.findHREF(i['src'])\n",
    "                    if ans:\n",
    "                        ans = self.findDomain(ans)\n",
    "                        #print(ans)\n",
    "                        if domain not in ans:\n",
    "                            phish += 1\n",
    "                except:\n",
    "                    continue\n",
    "        \n",
    "            for i in videos:\n",
    "                try:\n",
    "                    ans = self.findHREF(i['src'])\n",
    "                    ans = self.findDomain(ans)\n",
    "                    #print(ans)\n",
    "                    if domain not in ans:\n",
    "                        phish += 1\n",
    "                except:\n",
    "                    continue\n",
    "        \n",
    "            for i in audios:\n",
    "                try:\n",
    "                    ans = self.findHREF(i['src'])\n",
    "                    ans = self.findDomain(ans)\n",
    "                    #print(ans)\n",
    "                    if domain not in ans:\n",
    "                        phish += 1\n",
    "                except:\n",
    "                    continue\n",
    "        \n",
    "            for i in sources:\n",
    "                try:\n",
    "                    ans = self.findHREF(i['src'])\n",
    "                    ans = self.findDomain(ans)\n",
    "                    #print(ans)\n",
    "                    if domain not in ans:\n",
    "                        phish += 1\n",
    "                except:\n",
    "                    continue\n",
    "                    \n",
    "            #calculating percentage of phishing Request_URL \n",
    "            try:\n",
    "                percent = (phish/count)*100\n",
    "                if percent < 22:\n",
    "                    Request_URL = 1\n",
    "                elif percent < 61:\n",
    "                    Request_URL = 0\n",
    "                else:\n",
    "                    Request_URL = -1\n",
    "                    \n",
    "            except ZeroDivisionError:\n",
    "                Request_URL = 0\n",
    "                \n",
    "                \n",
    "            for link in anchors:\n",
    "                hrf = link.get('href')\n",
    "                #print(hrf)\n",
    "                hrf = self.findHREF(url)\n",
    "                if hrf != []:\n",
    "                    hrf = self.findDomain(hrf)\n",
    "                    if domain not in hrf:\n",
    "                        anchor_phish += 1\n",
    "            \n",
    "            #Calculating percentage of phishing URLs in anchor tags\n",
    "            try:\n",
    "                anchor_phish_percent = (anchor_phish/anchor_count)*100 \n",
    "                if anchor_phish_percent < 31:\n",
    "                    URL_of_Anchor = 1\n",
    "                elif anchor_phish_percent < 68:\n",
    "                    URL_of_Anchor = 0\n",
    "                else:\n",
    "                    URL_of_Anchor = -1\n",
    "                    \n",
    "            except ZeroDivisionError:\n",
    "                URL_of_Anchor = 0\n",
    "    \n",
    "        except:\n",
    "            Request_URL = -1\n",
    "            URL_of_Anchor = -1\n",
    "    \n",
    "        #print(Request_URL)\n",
    "        #print(URL_of_Anchor)\n",
    "        self.Request_URL = Request_URL\n",
    "        self.URL_of_Anchor = URL_of_Anchor\n",
    "        #print(count,anchor_count)\n",
    "\n",
    "    \n",
    "    def findLinksInTags(self, url):\n",
    "        \n",
    "        \"\"\"\n",
    "        % of Links in \" < Meta > \",\" < Script > \" and \" < Link>\" < 17% → Legitimate\n",
    "        % of Links in < Meta > \",\" < Script > \" and \" < Link>\" ≥ 17% And ≤ 81% → Suspicious\n",
    "        Otherwise → Phishing\n",
    "        \n",
    "        \"\"\"\n",
    "        domain = self.findDomain(url)\n",
    "        try:\n",
    "            req = requests.get(url)\n",
    "            soup = BeautifulSoup(req.content, 'html.parser',from_encoding=\"iso-8859-1\")\n",
    "            links  = soup.find_all('link')\n",
    "            scripts = soup.find_all('script')\n",
    "            metas = soup.find_all('meta')\n",
    "            metas_count = 0\n",
    "            phish_tags = 0\n",
    "\n",
    "            for i in links:\n",
    "                hrf = i.get('href')\n",
    "                hrf = self.findHREF(str(hrf))\n",
    "                #print(hrf)\n",
    "                if hrf:\n",
    "                    hrf = self.findDomain(hrf)\n",
    "                    if domain not in hrf:\n",
    "                        phish_tags += 1\n",
    "\n",
    "\n",
    "\n",
    "            for i in scripts:\n",
    "                try:\n",
    "                    hrf = i.get('src')\n",
    "                    hrf = self.findHREF(str(hrf))\n",
    "                    #print(hrf)\n",
    "                    if hrf:\n",
    "                        hrf = self.findDomain(hrf)\n",
    "                        if domain not in hrf:\n",
    "                            phish_tags += 1\n",
    "                except:\n",
    "                    continue\n",
    "\n",
    "\n",
    "\n",
    "            for i in metas:\n",
    "                try:\n",
    "                    hrf = i.get('content')\n",
    "                    hrf = self.findHREF(str(hrf))\n",
    "                    #print(hrf)\n",
    "                    if hrf:\n",
    "                        metas_count += 1\n",
    "                        hrf = self.findDomain(hrf)\n",
    "                        if domain not in hrf:\n",
    "                            phish_tags += 1\n",
    "                except:\n",
    "                    continue\n",
    "\n",
    "            total_link = len(links) + len(scripts) + metas_count\n",
    "            #print(total_link)\n",
    "            try:\n",
    "                Links_in_tag_percent = (phish_tags/total_link)*100\n",
    "                if Links_in_tag_percent < 17:\n",
    "                    Links_in_tag = 1\n",
    "                elif Links_in_tag_percent <= 81:\n",
    "                    Links_in_tag = 0\n",
    "                else:\n",
    "                    Links_in_tag = -1\n",
    "            \n",
    "            except ZeroDivisionError:\n",
    "                Links_in_tag = 0\n",
    "\n",
    "        except:\n",
    "            Links_in_tag = -1\n",
    "\n",
    "        \n",
    "        self.Links_in_tags = Links_in_tag\n",
    "        \n",
    "    def findSFH(self,url):\n",
    "        \n",
    "        \"\"\"\n",
    "        SFH is \"about: blank\" Or Is Empty → Phishing : -1\n",
    "        SFH Refers To A Different Domain → Suspicious : 0\n",
    "        Otherwise → Legitimate : 1\n",
    "\n",
    "        \"\"\"\n",
    "        domain = self.findDomain(url)\n",
    "        try:\n",
    "            req = requests.get(url)\n",
    "            soup = BeautifulSoup(req.content, 'html.parser',from_encoding=\"iso-8859-1\")\n",
    "            form = soup.find_all('form')\n",
    "            #print(form)\n",
    "            for i in form:\n",
    "                hrf = i.get('action')\n",
    "                if not hrf or hrf.lower() == 'empty':\n",
    "                    SFH = -1\n",
    "                else:\n",
    "                    hrf = self.findHREF(hrf)\n",
    "                    if hrf:\n",
    "                        hrf = self.findDomain(hrf)\n",
    "                        if domain in hrf:\n",
    "                            SFH = 1\n",
    "                        else:\n",
    "                            SFH = 0\n",
    "                    else:\n",
    "                        SFH = 1\n",
    "            if form == []:\n",
    "                SFH = 0\n",
    "\n",
    "        except:\n",
    "            SFH = -1\n",
    "            \n",
    "        self.SFH = SFH\n",
    "        \n",
    "    def getAllFeatures(self):\n",
    "        \n",
    "        custom = [{ 'SSLfinal_State': self.SSLfinal_State,\n",
    "         'URL_of_Anchor':self.URL_of_Anchor,\n",
    "         'Prefix_Suffix': self.Prefix_Suffix,\n",
    "         'web_traffic': self.web_traffic,\n",
    "         'having_Sub_Domain': self.having_Sub_Domain,\n",
    "         'Request_URL': self.Request_URL,\n",
    "         'Links_in_tags': self.Links_in_tags,\n",
    "         'SFH':self.SFH }]\n",
    "        to_predict = pd.DataFrame.from_dict(custom) \n",
    "        #print(to_predict.to_string())\n",
    "        return to_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter number of websites you want to check: 3\n",
      "dsfssf\n",
      "!! Please feed a proper URL !!\n",
      "\n",
      "http://www.paypal.com.cgi.bin.webscr.cmd.login.submit.dispatch.5885d80a13c03faee8dcbcd55a50598f04d34b4bf5tt1.mediareso.com/secure-code90/security/\n",
      "XX Phishing XX \n",
      "\n",
      "https://www.netflix.com/watch/80028080?trackId=155573560\n",
      "|| Legitimate ||\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import Algorithm\n",
    "def main():\n",
    "    num = int(input(\"Enter number of websites you want to check: \"))\n",
    "    for i in range(num):\n",
    "        input_url = input()\n",
    "        url = URL(input_url)\n",
    "        flag = url.getFlag()\n",
    "        if flag:\n",
    "            to_predict = url.getAllFeatures()\n",
    "            predicted = Algorithm.rfc.predict(to_predict)\n",
    "            if predicted[0] == 1:\n",
    "                print('|| Legitimate ||')\n",
    "            else:\n",
    "                print(\"XX Phishing XX \")\n",
    "        print()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
